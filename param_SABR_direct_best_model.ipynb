{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ANN learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as si\n",
    "from scipy.stats import norm\n",
    "from functools import partial\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Data_SABR_vol_mult_rst.txt\", header=None)\n",
    "# data headings: [v, alpha, rho, imp vol 1-36]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inputs and targets\n",
    "targets = data.iloc[:, 0:3]\n",
    "data = data.iloc[:, 3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
    "    data, targets, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store test inputs and targets for resultsDataFrame(test_targets).to_csv(\"test_targetsSABRparam.csv\")\n",
    "pd.DataFrame(test_inputs).to_csv(\"test_inputsSABRparam.csv\")\n",
    "pd.DataFrame(test_targets).to_csv(\"test_targetsSABRparam.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensor\n",
    "train_inputs = tf.convert_to_tensor(train_inputs, dtype='float64')\n",
    "train_targets = tf.convert_to_tensor(train_targets.values, dtype='float64')\n",
    "test_inputs = tf.convert_to_tensor(test_inputs, dtype='float64')\n",
    "test_targets = tf.convert_to_tensor(test_targets.values, dtype='float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 3 \n",
    "input_size = train_inputs.shape[1]\n",
    "\n",
    "hidden_layer_size = 249\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(\n",
    "        input_size, activation=tf.nn.relu),  # 1st hidden layer\n",
    "    tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation=tf.nn.relu),  # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation=tf.nn.relu),  # 3rd Hidden layer\n",
    "    tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation=tf.nn.relu),  # 4th Hidden layer\n",
    "    tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation=tf.nn.relu),  # 5th Hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation=\"linear\")  # output layer\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an early stopping mechanism\n",
    "LRScheduler = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=15,\n",
    "                                verbose=1, min_delta=0.0001, cooldown=5, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create customized loss function\n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    squared_difference = squared_difference * [0.25, 0.25,40]\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define separate model\n",
    "units_1=[26,26,26,26,256]\n",
    "units_2=[16,16,16,16,256]\n",
    "units_3=[195,195,195,195,195,195,256]\n",
    "input= Input(shape=(36,))\n",
    "\n",
    "x_1=Dense(units_1[0], activation=\"relu\")(input)\n",
    "#x_1=BatchNormalization()(x_1)\n",
    "for i in range(1, len(units_1)-1):\n",
    "    x_1=Dense(units_1[i], activation=\"relu\")(x_1)\n",
    "    #x_1=BatchNormalization()(x_1)\n",
    "\n",
    "x_2=Dense(units_2[0], activation=\"relu\")(input)\n",
    "#x_2=BatchNormalization()(x_2)\n",
    "for i in range(1, len(units_2)-1):\n",
    "    x_2=Dense(units_2[i], activation=\"relu\")(x_2)\n",
    "   # x_2=BatchNormalization()(x_2)\n",
    "x_3=Dense(units_3[0], activation=\"relu\")(input)\n",
    "#x_2=BatchNormalization()(x_2)\n",
    "for i in range(1, len(units_3)-1):\n",
    "    x_3=Dense(units_3[i], activation=\"relu\")(x_3)\n",
    "   # x_2=BatchNormalization()(x_2)\n",
    "\n",
    "x=Concatenate()([x_1,x_2,x_3])\n",
    "x=Dense(256, activation=\"relu\")(x)\n",
    "\n",
    "output=Dense(3)(x)\n",
    "\n",
    "\n",
    "model=Model(inputs=[input], outputs=[output])\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set early stopping mechanism\n",
    "LRScheduler = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=15,\n",
    "                                verbose=1, min_delta=0.00001, cooldown=5, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin training\n",
    "adam = Adam(lr=2e-05)\n",
    "model.compile(optimizer=adam, loss=my_loss_fn, metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "batch_size = 216\n",
    "\n",
    "max_epochs = 50000\n",
    "\n",
    "\n",
    "history = model.fit(train_inputs,  \n",
    "                    train_targets,  \n",
    "                    batch_size=batch_size,  \n",
    "                    epochs=max_epochs,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[\n",
    "                        keras.callbacks.EarlyStopping(monitor='val_loss', patience=40,min_delta=0.0000001),\n",
    "                        LRScheduler\n",
    "                        ],\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predicted parameters\n",
    "nu_pred = model.predict(test_inputs)[0:1000, 0]\n",
    "pd.DataFrame(nu_pred).to_csv(\"SABR_nu_pred.csv\")\n",
    "alpha_pred = model.predict(test_inputs)[0:1000, 1]\n",
    "pd.DataFrame(alpha_pred).to_csv(\"SABR_alpha_pred.csv\")\n",
    "rho_pred = model.predict(test_inputs)[0:1000, 2]\n",
    "pd.DataFrame(rho_pred).to_csv(\"SABR_rho_pred.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse train and validation across epochs\n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1, len(history.epoch)+1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training Mean Square Error')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation Mean Square Error')\n",
    "plt.title('Training and Validation Mean Square Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mse train and validation across epochs\n",
    "loss_train = history.history[\"mean_absolute_error\"]\n",
    "loss_val = history.history[\"val_mean_absolute_error\"]\n",
    "epochs = range(1, len(history.epoch)+1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training Mean Absolute Error')\n",
    "plt.plot(epochs, loss_val, 'b', label='Validation Mean Absolute Error')\n",
    "plt.title('Training and Validation Mean Absolute Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "model.evaluate(test_inputs ,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_dir = 'paramSABRModel.h5'\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "calibration_model = load_model('paramSABRModel.h5', custom_objects={\n",
    "                               'my_loss_fn': my_loss_fn})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time pre-trained model on 1000 samples\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "s = (1000, output_size)\n",
    "y_pred = np.zeros(s)\n",
    "for i in range(1000):\n",
    "    X_try = test_inputs[i, :]\n",
    "    c = np.array([X_try])\n",
    "    y_pred[i, :] = calibration_model.predict(c)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# total time taken\n",
    "print(f\"Runtime of the program is {end - start}\")\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv(\"SABR_param_pred.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save true targets\n",
    "test_targets = test_targets.numpy()\n",
    "test_targets_sub = test_targets[0:999, :]\n",
    "test_targets_sub\n",
    "pd.DataFrame(test_targets_sub).to_csv(\"SABR_param_true.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7595ccbf68387614422bee2164a42c53f7d32c50f8b886591783b6dae9c7ad8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
