{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow libraries\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bayesian optimisation libraries\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameter tuning upper and lower bounds\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=6, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n",
    "\n",
    "dim_num_batch = Integer(low=32, high=512, name='num_batch')\n",
    "dim_dropout = Categorical(categories=[True, False], name='num_dropout')\n",
    "dim_batchnorm = Categorical(categories=[True, False], name='num_batchnorm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all hyperparameters that require tuning\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_num_batch, dim_dropout, dim_batchnorm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use manual hyperparameter selection for default parameters\n",
    "default_parameters = [1e-5, 1, 16, 64, True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define log directory to view models\n",
    "def log_dir_name(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, num_batch, num_dropout, num_batchnorm):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./Bayes_Hest_impvol_multi_logs/lr_{0:.0e}_layers_{1}_nodes_{2}_batch_{3}_dropout_{4}_batchNorm_{5}/\"\n",
    "\n",
    "    log_dir = s.format(learning_rate,\n",
    "                       num_dense_layers,\n",
    "                       num_dense_nodes,\n",
    "                       num_batch, num_dropout, num_batchnorm)\n",
    "\n",
    "    return log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Data_heston_rstparm2.txt\", header=None)\n",
    "targets = data.iloc[:, 5:]\n",
    "data = data.iloc[:, 0:5]\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
    "    data, targets, test_size=0.3)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_inputs)\n",
    "train_inputs = scaler.transform(train_inputs)\n",
    "test_inputs = scaler.transform(test_inputs)\n",
    "train_inputs = tf.convert_to_tensor(train_inputs, dtype='float64')\n",
    "train_targets = tf.convert_to_tensor(train_targets.values, dtype='float64')\n",
    "test_inputs = tf.convert_to_tensor(test_inputs, dtype='float64')\n",
    "test_targets = tf.convert_to_tensor(test_targets.values, dtype='float64')\n",
    "input_size = train_inputs.shape[1]\n",
    "output_size = 36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopping mechanism\n",
    "LRScheduler = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10,\n",
    "                                verbose=0, min_delta=0.001, cooldown=10, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, num_batch, num_dropout, num_batchnorm):\n",
    "\n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(input_size,)))\n",
    "\n",
    "    for i in range(num_dense_layers):\n",
    "        # Name of the layer. \n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "    \n",
    "        if num_batchnorm:\n",
    "            model.add(BatchNormalization())\n",
    "        if num_dropout:\n",
    "            model.add(Dropout(0.1))\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                        activation=\"relu\",\n",
    "                        name=name))\n",
    "\n",
    " \n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "\n",
    "    # Find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    # In Keras we need to compile the model so it can be trained.\n",
    "    model.compile(optimizer=\"adam\", loss='mse',\n",
    "                  metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise loss and best model directory\n",
    "best_loss = 99999999\n",
    "best_model_dir = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimisation objective function\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers,\n",
    "            num_dense_nodes, num_batch, num_dropout, num_batchnorm):\n",
    "  \n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('num_batch:', num_batch)\n",
    "    print('num_dropout:', num_dropout)\n",
    "    print('num_batchnorm:', num_batchnorm)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model(learning_rate=learning_rate,\n",
    "                         num_dense_layers=num_dense_layers,\n",
    "                         num_dense_nodes=num_dense_nodes,\n",
    "                         num_batch=num_batch, num_dropout=num_dropout, num_batchnorm=num_batchnorm)\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(learning_rate, num_dense_layers,\n",
    "                           num_dense_nodes, num_batch, num_dropout, num_batchnorm)\n",
    "\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir)\n",
    "\n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(train_inputs,  \n",
    "                        train_targets,  \n",
    "                        batch_size=num_batch,  \n",
    "                        epochs=250,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=False, min_delta=0.000001),\n",
    "                                   LRScheduler, callback_log],\n",
    "                        verbose=True)\n",
    "\n",
    "    loss = history.history['val_loss'][-1]\n",
    "\n",
    "    # Print the loss\n",
    "    print(\"Loss: {0}\".format(loss))\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    global best_loss\n",
    "    global best_model_dir\n",
    "\n",
    "    # If the loss of the saved model is improved \n",
    "    if loss < best_loss:\n",
    "        # Save the new model \n",
    "        if best_model_dir == '':\n",
    "          pass\n",
    "        else:\n",
    "          os.remove(best_model_dir)\n",
    "\n",
    "        best_model_dir = os.path.join(log_dir, 'best_model.h5')\n",
    "        model.save(best_model_dir)\n",
    "        # Update the loss\n",
    "        best_loss = loss\n",
    "\n",
    "    print(best_model_dir)\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "  \n",
    "    K.clear_session()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function is working\n",
    "fitness(x=default_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin bayesian optimization\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=40,\n",
    "                            x0=default_parameters)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7595ccbf68387614422bee2164a42c53f7d32c50f8b886591783b6dae9c7ad8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
