{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Input\n",
    "from tensorflow.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow libraries\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bayesian optimisation libraries\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameter tuning upper and lower bounds\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n",
    "dim_num_batch = Integer(low=32, high=512, name='num_batch')\n",
    "dim_dropout = Categorical(categories=[True, False],name='num_dropout')\n",
    "dim_batchnorm = Categorical(categories=[True, False], name='num_batchnorm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all hyperparameters that require tuning\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_num_batch, dim_dropout, dim_batchnorm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use manual hyperparameter selection for default parameters\n",
    "default_parameters = [1e-5, 1, 16, 64,True,False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define log directory to view models\n",
    "def log_dir_name(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, num_batch, num_dropout, num_batchnorm):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./19_logs/lr_{0:.0e}_layers_{1}_nodes_{2}_{3}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(learning_rate,\n",
    "                       num_dense_layers,\n",
    "                       num_dense_nodes,\n",
    "                       num_batch, num_dropout, num_batchnorm)\n",
    "\n",
    "    return log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Data_SABR.txt\", header=None, names=[\n",
    "                   \"K\", \"T\", \"v\", \"alpha\", \"rho\", \"imp vol\"])\n",
    "targets = data.iloc[:, 5]\n",
    "data = data.iloc[:, 0:5]\n",
    "data\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
    "    data, targets, test_size=0.3)\n",
    "train_inputs = tf.convert_to_tensor(train_inputs, dtype='float64')\n",
    "train_targets = tf.convert_to_tensor(train_targets.values, dtype='float64')\n",
    "test_inputs = tf.convert_to_tensor(test_inputs, dtype='float64')\n",
    "test_targets = tf.convert_to_tensor(test_targets.values, dtype='float64')\n",
    "input_size = train_inputs.shape[1]\n",
    "output_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopping mechanism\n",
    "LRScheduler = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10,\n",
    "                                verbose=0, min_delta=0.001, cooldown=10, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, num_batch, num_dropout, num_batchnorm):\n",
    "  \n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(input_size,)))\n",
    "\n",
    "\n",
    "    for i in range(num_dense_layers):\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "        if num_batchnorm:model.add(BatchNormalization())\n",
    "        if num_dropout:model.add(Dropout(0.1))\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                        activation=\"relu\",\n",
    "                        name=name))\n",
    "\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "\n",
    "    # Find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    # Compile the model so it can be trained.\n",
    "    model.compile(optimizer=\"adam\", loss='mse', metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise loss and best model directory\n",
    "best_accuracy = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimisation objective function\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers,\n",
    "            num_dense_nodes, num_batch, num_dropout, num_batchnorm):\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('num_batch:', num_batch)\n",
    "    print('num_dropout:', num_dropout)\n",
    "    print('num_batchnorm:', num_batchnorm)\n",
    "\n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model(learning_rate=learning_rate,\n",
    "                         num_dense_layers=num_dense_layers,\n",
    "                         num_dense_nodes=num_dense_nodes,\n",
    "                         num_batch=num_batch, num_dropout=num_dropout, num_batchnorm=num_batchnorm)\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(learning_rate, num_dense_layers,\n",
    "                           num_dense_nodes, num_batch, num_dropout, num_batchnorm)\n",
    "\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False)\n",
    "\n",
    "    # Use Keras to train the model.\n",
    "    history=model.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size=num_batch,  \n",
    "          epochs=250,\n",
    "          validation_split = 0.1,\n",
    "           callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=False, min_delta=0.000001),\n",
    "                          LRScheduler],\n",
    "          verbose=True)\n",
    "\n",
    "    # Get the loss on the validation-set\n",
    "    accuracy = history.history['val_loss'][-1]\n",
    "\n",
    "    # Print the loss\n",
    "    print(\"Loss: {0}\".format(accuracy))\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the loss of the saved model is improved \n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model \n",
    "        model.save('models/best_model.h5')\n",
    "\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function is working\n",
    "fitness(x=default_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin bayesian optimization\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=40,\n",
    "                            x0=default_parameters)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7595ccbf68387614422bee2164a42c53f7d32c50f8b886591783b6dae9c7ad8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
