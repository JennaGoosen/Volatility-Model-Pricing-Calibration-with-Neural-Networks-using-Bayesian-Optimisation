{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bayesian optimisation libraries\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameter tuning upper and lower bounds\n",
    "dim_learning_rate = Real(low=1e-5, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "dim_num_dense_layers1 = Integer(low=1, high=5, name='num_dense_layers1')\n",
    "dim_num_dense_layers2 = Integer(low=1, high=5, name='num_dense_layers2')\n",
    "dim_num_dense_layers3 = Integer(low=1, high=7, name='num_dense_layers3')\n",
    "dim_num_dense_nodes1 = Integer(low=5, high=256, name='num_dense_nodes1')\n",
    "dim_num_dense_nodes2 = Integer(low=5, high=256, name='num_dense_nodes2')\n",
    "dim_num_dense_nodes3 = Integer(low=5, high=256, name='num_dense_nodes3')\n",
    "dim_num_batch = Integer(low=16, high=512, name='num_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all hyperparameters that require tuning\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers1, dim_num_dense_layers2, dim_num_dense_layers3,\n",
    "              dim_num_dense_nodes1, dim_num_dense_nodes2, dim_num_dense_nodes3,\n",
    "              dim_num_batch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use manual hyperparameter selection for default parameters\n",
    "default_parameters = [1e-5, 3,3,5, 16,250,250, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define log directory to view models\n",
    "def log_dir_name(learning_rate,\n",
    "                 num_dense_layers1, num_dense_layers2, num_dense_layers3,\n",
    "                 num_dense_nodes1, num_dense_nodes2, num_dense_nodes3,\n",
    "                 num_batch):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"./Bayes_SABR_param_sep_logs/lr_{0:.0e}_layer1_{1}_layer2_{2}_layer3_{3}_nodes1_{4}_nodes2_{5}_nodes3_{6}_batch_{7}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(learning_rate,\n",
    "                       num_dense_layers1, num_dense_layers2, num_dense_layers3,\n",
    "                       num_dense_nodes1, num_dense_nodes2, num_dense_nodes3,\n",
    "                       num_batch)\n",
    "\n",
    "    return log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Data_SABR_vol_mult_rst.txt\", header=None)\n",
    "targets = data.iloc[:, 0:3]\n",
    "data = data.iloc[:, 3:]\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
    "    data, targets, test_size=0.3,shuffle=False)\n",
    "train_inputs = tf.convert_to_tensor(train_inputs, dtype='float64')\n",
    "train_targets = tf.convert_to_tensor(train_targets.values, dtype='float64')\n",
    "test_inputs = tf.convert_to_tensor(test_inputs, dtype='float64')\n",
    "test_targets = tf.convert_to_tensor(test_targets.values, dtype='float64')\n",
    "input_size = train_inputs.shape[1]\n",
    "output_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopping mechanism\n",
    "LRScheduler = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10,\n",
    "                                verbose=0, min_delta=0.000001, cooldown=10, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define customized loss function\n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    squared_difference = squared_difference * [0.25, 0.25, 0.5]\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, num_dense_layers1, num_dense_layers2, num_dense_layers3,\n",
    "                    num_dense_nodes1, num_dense_nodes2, num_dense_nodes3,num_batch):\n",
    "   \n",
    "     \n",
    "        input = Input(shape=(36,))\n",
    "        \n",
    "        x_1 = Dense(num_dense_nodes1, activation=\"relu\")(input)\n",
    "        for i in range(1,num_dense_layers1-1):\n",
    "\n",
    "            x_1 =Dense(num_dense_nodes1,\n",
    "                            activation=\"relu\")(x_1)\n",
    "        x_2 = Dense(num_dense_nodes2, activation=\"relu\")(input)\n",
    "        for i in range(num_dense_layers2-1):\n",
    "\n",
    "            x_2 = Dense(num_dense_nodes2,\n",
    "                        activation=\"relu\")(x_2)\n",
    "        x_3 = Dense(num_dense_nodes3,activation=\"relu\")(input)\n",
    "        for i in range(num_dense_layers3-1):\n",
    "            x_3 = Dense(num_dense_nodes3,\n",
    "                        activation=\"relu\")(x_3)\n",
    "\n",
    "\n",
    "        x = Concatenate()([x_1, x_2, x_3])\n",
    "        x = Dense(8, activation=\"relu\")(x)\n",
    "        output = Dense(3)(x)\n",
    "        model = Model(inputs=[input], outputs=[output])\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "    # Compile the model so it can be trained.\n",
    "        model.compile(optimizer=\"adam\", loss=my_loss_fn,\n",
    "                metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise loss and best model directory\n",
    "best_loss = 99999999\n",
    "best_model_dir = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimisation objective function\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers1, num_dense_layers2, num_dense_layers3, num_dense_nodes1, num_dense_nodes2, num_dense_nodes3, num_batch):\n",
    "\n",
    "  # Print the hyper-parameters.\n",
    "  print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "  print('num_dense_layers1:', num_dense_layers1)\n",
    "  print('num_dense_layers2:', num_dense_layers2)\n",
    "  print('num_dense_layers3:', num_dense_layers3)\n",
    "  print('num_dense_nodes1:', num_dense_nodes1)\n",
    "  print('num_dense_nodes2:', num_dense_nodes2)\n",
    "  print('num_dense_nodes3:', num_dense_nodes3)\n",
    "  print('num_batch:', num_batch)\n",
    "\n",
    "  # Create the neural network with these hyper-parameters.\n",
    "  model = create_model(learning_rate=learning_rate,\n",
    "                        num_dense_layers1=num_dense_layers1, num_dense_layers2=num_dense_layers2, num_dense_layers3=num_dense_layers3,\n",
    "                        num_dense_nodes1=num_dense_nodes1, num_dense_nodes2=num_dense_nodes2, num_dense_nodes3=num_dense_nodes3,\n",
    "                        num_batch=num_batch)\n",
    "    \n",
    "  # Dir-name for the TensorBoard log-files.\n",
    "  log_dir=log_dir_name(learning_rate, num_dense_layers1, num_dense_layers2, num_dense_layers3,\n",
    "                          num_dense_nodes1, num_dense_nodes2, num_dense_nodes3, num_batch)\n",
    "\n",
    "\n",
    "\n",
    "  callback_log = TensorBoard(\n",
    "      log_dir=log_dir)\n",
    "\n",
    "  # Use Keras to train the model.\n",
    "  history = model.fit(train_inputs,  \n",
    "                      train_targets,  \n",
    "                      batch_size=num_batch,  \n",
    "                      epochs=250,\n",
    "                      validation_split=0.1,\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, verbose=False, min_delta=0.000001),\n",
    "                                  LRScheduler,\n",
    "                                  callback_log],\n",
    "                    \n",
    "                      verbose=True)\n",
    "\n",
    "  # Get the loss on the validation-set\n",
    "  loss = history.history['val_loss'][-1]\n",
    "\n",
    "  # Print the loss\n",
    "  print(\"Loss: {0}\".format(loss))\n",
    "\n",
    "  # Save the model if it improves on the best-found performance.\n",
    "  global best_loss\n",
    "  global best_model_dir\n",
    "  # If the lpss of the saved model is improved.\n",
    "  if loss < best_loss:\n",
    "      # Save the new model\n",
    "      if best_model_dir == '':\n",
    "        pass\n",
    "      else:\n",
    "        os.remove(best_model_dir)\n",
    "\n",
    "      best_model_dir = os.path.join(log_dir, 'best_model.h5')\n",
    "      model.save(best_model_dir)\n",
    "      # Update the loss\n",
    "      best_loss = loss\n",
    "\n",
    "  print(best_model_dir)\n",
    "  # Delete the Keras model with these hyper-parameters from memory.\n",
    "  del model\n",
    "\n",
    "  K.clear_session()\n",
    "\n",
    "  return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function is working\n",
    "fitness(x=default_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin bayesian optimization\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=40,\n",
    "                            x0=default_parameters)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7595ccbf68387614422bee2164a42c53f7d32c50f8b886591783b6dae9c7ad8d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
